{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SVA9v_JTq_-"
      },
      "source": [
        "# 1. Presto to Vertex AI\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/nasaharvest/presto/blob/initial-deploy-code/deploy/1_Presto_to_VertexAI.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "**Authors**: Ivan Zvonkov, Gabriel Tseng, (additional credits: [Earth_Engine_PyTorch_Vertex_AI](https://github.com/google/earthengine-community/blob/master/guides/linked/Earth_Engine_PyTorch_Vertex_AI.ipynb))\n",
        "\n",
        "**Description**: The notebook Deploys Presto to Vertex AI. This is a prerequisite to generating Presto embeddings on Google Earth Engine using\n",
        "[ee.Model.fromVertexAi](https://developers.google.com/earth-engine/apidocs/ee-model-fromvertexai).\n",
        "\n",
        "Once the model is deployed this [GEE script](https://code.earthengine.google.com/df6348b8d47cd751eb5164dccb7b26a9) can be used to generate Presto embeddings.\n",
        "\n",
        "**Steps**:\n",
        "1. Set up environment\n",
        "2. Load default Presto model\n",
        "3. Transform Presto model into TorchScript\n",
        "4. Package TorchScript model into TorchServe\n",
        "5. Deploy and use Vertex AI\n",
        "\n",
        "    5a. Upload TorchServe model to Vertex AI Model Registry [Free]\n",
        "\n",
        "    5b. Create a Vertex AI Endpoint [Free]\n",
        "\n",
        "    5c. Deploy model to endpoint [Cost depends on Minimum Replica Count parameter]\n",
        "\n",
        "    5d. Generate embeddings in Google Earth Engine [Cost depends on region size]\n",
        "\n",
        "    5e. Undeploy model from endpoint [Free]\n",
        "\n",
        "**Cost Breakdown**:\n",
        "\n",
        "*5a. Upload TorchServe model to Vertex AI Model Registry [Free]*\n",
        "- Model files are uploaded to Cloud Storage but are lightweight (3.37 Mb total) and thus easily fall into Google Cloud's 5GB/month Storage [Free Tier](https://cloud.google.com/storage/pricing#cloud-storage-always-free)\n",
        "- There is no cost to storing models in Vertex AI Model Registry ([source](https://cloud.google.com/vertex-ai/pricing#modelregistry))\n",
        "\n",
        "*5b. Create a Vertex AI Endpoint [Free]*\n",
        "- There is no cost to creating an endpoint. Costs start when a model is deployed to that endpoint\n",
        "\n",
        "*5c. Deploy model to endpoint [Cost depends on Minimum Replica Count parameter]*\n",
        "- The `Minimum Replica Count` represents the minimum amount of compute nodes started when a model is deployed is e2-standard-2 machine (\\$0.0771/node hour in us-central-1)\n",
        "- So as long as the endpoint is active you will be paying \\$0.0771/hour even if no predictions are made\n",
        "\n",
        "*5d. Generate embeddings in Google Earth Engine [Cost depends on region size]*\n",
        "- Once a model is deployed and `ee.model.fromVertexAi` is used Vertex AI scales the amount of nodes based on amount of data (size of the region)\n",
        "- Our current embedding generation cost estimates are <strong>\\$5.37 - \\$10.14 per 1000 km<sup>2</sup> </strong>\n",
        "- We compute a cost estimate for your ROI in our Google Earth Engine script\n",
        "\n",
        "*5e. Undeploy model from endpoint [Free]*\n",
        "- Necessary to stop incurring charges from 5c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzb1bwgTUZU0"
      },
      "source": [
        "## 1. Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuoEjld3TTLO"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUI_5pWJ3V4s",
        "outputId": "25e8a556-7401-45c0-f1ef-9c02b53948d9"
      },
      "outputs": [],
      "source": [
        "# REPLACE WITH YOUR CLOUD PROJECT!\n",
        "PROJECT = 'presto-deployment'\n",
        "!gcloud config set project {PROJECT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRGjYjltsm6-",
        "outputId": "7ac7b255-bea0-4d90-9baf-6157ba17aa26"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nasaharvest/presto.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zGbf2KIhLA"
      },
      "source": [
        "## 2. Load default Presto model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKgCxBNnYJIB",
        "outputId": "f2384124-4109-4031-87df-95140c7d7029"
      },
      "outputs": [],
      "source": [
        "# Navigate inside of the repository to import Presto\n",
        "%cd /content/presto\n",
        "\n",
        "import torch\n",
        "from presto.single_file_presto import Presto\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Presto.construct()\n",
        "model.load_state_dict(torch.load(\"data/default_model.pt\", map_location=device))\n",
        "model.eval();\n",
        "\n",
        "# Navigate back to main directory\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7gJoysFTsf"
      },
      "source": [
        "## 3. Transform Presto model into TorchScript\n",
        "> TorchScript is a way to create serializable and optimizable models from PyTorch code.\n",
        "https://docs.pytorch.org/docs/stable/jit.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDKqqzi9F7T4"
      },
      "outputs": [],
      "source": [
        "# Construct input manually\n",
        "batch_size = 256\n",
        "\n",
        "X_tensor = torch.zeros([batch_size, 12, 17])\n",
        "latlons_tensor = torch.zeros([batch_size, 2])\n",
        "\n",
        "dw_empty = torch.full([batch_size, 12], 9, device=device).long()\n",
        "month_tensor = torch.full([batch_size], 1, device=device)\n",
        "\n",
        "# [0   1   2   3   4   5   6   7   8   9    10   11   12    13      14    16     17  ]\n",
        "# [VV, VH, B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12, temp, precip, elev, slope, NDVI]\n",
        "mask = torch.zeros(X_tensor.shape, device=device).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdSXGZuikHUk"
      },
      "outputs": [],
      "source": [
        "# Verify forward pass with regular model\n",
        "with torch.no_grad():\n",
        "    preds = model.encoder(\n",
        "        x=X_tensor,\n",
        "        dynamic_world=dw_empty,\n",
        "        latlons=latlons_tensor,\n",
        "        mask=mask,\n",
        "        month=month_tensor\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFRvUVkHowKr",
        "outputId": "8704f15d-6af0-4c46-c4ee-82fe56881230"
      },
      "outputs": [],
      "source": [
        "# Make model torchscriptable\n",
        "example_kwargs = {\n",
        "    'x': X_tensor,\n",
        "    'dynamic_world': dw_empty,\n",
        "    'latlons': latlons_tensor,\n",
        "    'mask': mask,\n",
        "    'month': month_tensor\n",
        "}\n",
        "sm = torch.jit.trace(model.encoder, example_kwarg_inputs=example_kwargs)\n",
        "\n",
        "!mkdir -p pytorch_model\n",
        "sm.save('pytorch_model/model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYuSPOyp1A0K",
        "outputId": "c97ef2f4-7715-48ba-8b82-3d21dc401fff"
      },
      "outputs": [],
      "source": [
        "jit_model = torch.jit.load('pytorch_model/model.pt')\n",
        "jit_model(**example_kwargs).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b0LsZpqnByv"
      },
      "source": [
        "## 4. Package TorchScript model into TorchServe\n",
        "> TorchServe is a performant, flexible and easy to use tool for serving PyTorch models in production.\n",
        "https://docs.pytorch.org/serve/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i70o_BZml9vs",
        "outputId": "439d43f5-02ed-4dfd-be86-a49c058e70cd"
      },
      "outputs": [],
      "source": [
        "!pip install torchserve torch-model-archiver -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htq2Ac95FJlk",
        "outputId": "79076ad1-7cde-44f2-8e4a-07f28599d29a"
      },
      "outputs": [],
      "source": [
        "%%writefile pytorch_model/custom_handler.py\n",
        "import logging\n",
        "import torch\n",
        "from ts.torch_handler.base_handler import BaseHandler\n",
        "import numpy as np\n",
        "\n",
        "# UPDATE BASED ON YOUR NEEDS\n",
        "########################################\n",
        "VERSION = \"v1\"\n",
        "START_MONTH = 3\n",
        "BATCH_SIZE = 256\n",
        "########################################\n",
        "\n",
        "def printh(text):\n",
        "    # Prepends HANDLER to each print statement to make it easier to find in logs.\n",
        "    print(f\"HANDLER {VERSION}: {text}\")\n",
        "\n",
        "class ClassifierHandler(BaseHandler):\n",
        "\n",
        "    def inference(self, data):\n",
        "        printh(\"Inference begin\")\n",
        "\n",
        "        # Data shape: [ num_pixels, composite_bands, 1, 1 ]\n",
        "        data = data[:, :, 0, 0]\n",
        "        printh(f\"Data shape {data.shape}\")\n",
        "\n",
        "        num_bands = 17\n",
        "        printh(f\"Num_bands {num_bands}\")\n",
        "\n",
        "        # Subtract first two latlon\n",
        "        num_timesteps = (data.shape[1] - 2) // num_bands\n",
        "        printh(f\"Num_timesteps {num_timesteps}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batches = torch.split(data, BATCH_SIZE, dim=0)\n",
        "\n",
        "            # month: An int or torch.Tensor describing the first month of the instances being passed. If an int, all instances in the batch are assumed to have the same starting month.\n",
        "            month_tensor = torch.full([BATCH_SIZE], START_MONTH, device=self.device)\n",
        "            printh(f\"Month: {START_MONTH}\")\n",
        "\n",
        "            # dynamic_world: torch.Tensor of shape [BATCH_SIZE, num_timesteps]. If no Dynamic World classes are available, this tensor should be filled with the value DynamicWorld2020_2021.class_amount (i.e. 9), in which case it is ignored.\n",
        "            dw_empty = torch.full([BATCH_SIZE, num_timesteps], 9, device=self.device).long()\n",
        "            printh(f\"DW {dw_empty[0]}\")\n",
        "\n",
        "            # mask: An optional torch.Tensor of shape [BATCH_SIZE, num_timesteps, bands]. mask[i, j, k] == 1 means x[i, j, k] is considered masked. If the mask is None, no values in x are ignored.\n",
        "            mask = torch.zeros((BATCH_SIZE, num_timesteps, num_bands), device=self.device).float()\n",
        "            printh(f\"Mask sample one timestep: {mask[0, 0]}\")\n",
        "\n",
        "            preds_list = []\n",
        "            for batch in batches:\n",
        "                padding = 0\n",
        "                if batch.shape[0] < BATCH_SIZE:\n",
        "                    padding = BATCH_SIZE - batch.shape[0]\n",
        "                    batch = torch.cat([batch, torch.zeros([padding, batch.shape[1]], device=self.device)])\n",
        "\n",
        "                # x: torch.Tensor of shape [BATCH_SIZE, num_timesteps, bands] where bands is described by NORMED_BANDS.\n",
        "                X_tensor = batch[:, 2:]\n",
        "                printh(f\"X {X_tensor.shape}\")\n",
        "\n",
        "                X_tensor_reshaped = X_tensor.reshape(BATCH_SIZE, num_timesteps, num_bands)\n",
        "                printh(f\"X sample one timestep: {X_tensor_reshaped[0, 0]}\")\n",
        "\n",
        "                # latlons: torch.Tensor of shape [BATCH_SIZE, 2] describing the latitude and longitude of each input instance.\n",
        "                latlons_tensor = batch[:, :2]\n",
        "\n",
        "                printh(\"SHAPES\")\n",
        "                printh(f\"X {X_tensor_reshaped.shape}\")\n",
        "                printh(f\"DW {dw_empty.shape}\")\n",
        "                printh(f\"Latlons {latlons_tensor.shape}\")\n",
        "                printh(f\"Mask {mask.shape}\")\n",
        "                printh(f\"Month {month_tensor.shape}\")\n",
        "\n",
        "                pred = self.model(\n",
        "                    x=X_tensor_reshaped,\n",
        "                    dynamic_world=dw_empty,\n",
        "                    latlons=latlons_tensor,\n",
        "                    mask=mask,\n",
        "                    month=month_tensor\n",
        "                )\n",
        "                pred_np = np.expand_dims(pred.numpy(), axis=[1,2])\n",
        "                if padding == 0:\n",
        "                    preds_list.append(pred_np[:])\n",
        "                else:\n",
        "                    preds_list.append(pred_np[:-padding])\n",
        "\n",
        "        [printh(f\"{p.shape}\") for p in preds_list]\n",
        "        preds = np.concatenate(preds_list)\n",
        "        printh(f\"Preds shape {preds.shape}\")\n",
        "        return preds\n",
        "\n",
        "    def handle(self, data, context):\n",
        "        self.context = context\n",
        "        printh(f\"Handle begin\")\n",
        "        input_tensor = self.preprocess(data)\n",
        "        printh(f\"Input_tensor shape {input_tensor.shape}\")\n",
        "        pred_out = self.inference(input_tensor)\n",
        "        printh(f\"Inference complete\")\n",
        "        return self.postprocess(pred_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Dgq5Ob5b1i",
        "outputId": "e0200fa3-b535-4da6-d476-7614603b8370"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import pytorch_model.custom_handler\n",
        "\n",
        "importlib.reload(pytorch_model.custom_handler)\n",
        "\n",
        "from pytorch_model.custom_handler import ClassifierHandler, VERSION\n",
        "\n",
        "# Test output\n",
        "data = torch.zeros([713, 206, 1, 1])\n",
        "handler = ClassifierHandler()\n",
        "handler.model = jit_model\n",
        "preds = handler.handle(data=data, context=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90TXNAnfF-TD"
      },
      "outputs": [],
      "source": [
        "!torch-model-archiver -f \\\n",
        "  --model-name model \\\n",
        "  --version 1.0 \\\n",
        "  --serialized-file 'pytorch_model/model.pt' \\\n",
        "  --handler 'pytorch_model/custom_handler.py' \\\n",
        "  --export-path pytorch_model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYK9g3r1qVru"
      },
      "source": [
        "## 5. Deploy and use Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH0JO_Jww5Ok"
      },
      "source": [
        "### 5a. Upload TorchServe model to Vertex AI Model Registry\n",
        "> The Vertex AI Model Registry is a central repository where you can manage the lifecycle of your ML models.\n",
        "https://cloud.google.com/vertex-ai/docs/model-registry/introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8m9UBy3GEvZ",
        "outputId": "5f67c5c5-caad-487b-97ed-f3d5255132cc"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = f'gs://presto-models/{VERSION}'\n",
        "!gsutil cp -r pytorch_model {MODEL_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AetRF8dcGraC",
        "outputId": "03589695-1e44-4f79-c8bf-23b5444a0f28"
      },
      "outputs": [],
      "source": [
        "REGION = 'us-central1'\n",
        "MODEL_NAME = f'model_{VERSION}'\n",
        "CONTAINER_IMAGE = 'us-docker.pkg.dev/vertex-ai/prediction/pytorch-cpu.2-4:latest'\n",
        "\n",
        "!gcloud ai models upload \\\n",
        "  --artifact-uri={MODEL_DIR} \\\n",
        "  --region={REGION} \\\n",
        "  --container-image-uri={CONTAINER_IMAGE} \\\n",
        "  --description={MODEL_NAME} \\\n",
        "  --display-name={MODEL_NAME} \\\n",
        "  --model-id={MODEL_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BXDtoITxY2T"
      },
      "source": [
        "### 5b. Create a Vertex AI Endpoint\n",
        "> To deploy a model for online prediction, you need an endpoint.\n",
        "https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhMK9aA73FzI",
        "outputId": "f0568a0b-e606-4605-fb68-cc41096096cd"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_NAME = 'vertex-pytorch-presto-endpoint'\n",
        "\n",
        "endpoints = !gcloud ai endpoints list --region={REGION} --format='get(DISPLAY_NAME)'\n",
        "\n",
        "if ENDPOINT_NAME in endpoints:\n",
        "    print(f\"Endpoint: '{ENDPOINT_NAME}' already exists skipping endpoint creation.\")\n",
        "else:\n",
        "    print(f\"Endpoint: '{ENDPOINT_NAME}' does not exist, creating.\")\n",
        "    !gcloud ai endpoints create \\\n",
        "    --display-name={ENDPOINT_NAME} \\\n",
        "    --endpoint-id={ENDPOINT_NAME} \\\n",
        "    --region={REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWODP6ccx8-3"
      },
      "source": [
        "### 5c. Deploy model to endpoint\n",
        "> Deploying a model associates physical resources with the model so that it can serve online predictions with low latency.\n",
        "https://cloud.google.com/vertex-ai/docs/general/deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI297v1mjtR8",
        "outputId": "ddb82445-430c-442e-9e3b-d7765491a93b"
      },
      "outputs": [],
      "source": [
        "# Deploy model to endpoint, this will start an e2-standard-2 machine which costs money\n",
        "print(\"Track model deployment progress and prediction logs:\")\n",
        "print(f\"https://console.cloud.google.com/vertex-ai/online-prediction/locations/{REGION}/endpoints/{ENDPOINT_NAME}?project={PROJECT}\\n\")\n",
        "\n",
        "# Can take updward of 25 minutes\n",
        "# If using for large region, set min-replica-count higher to save scaling time\n",
        "!gcloud ai endpoints deploy-model {ENDPOINT_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --model={MODEL_NAME} \\\n",
        "    --display-name={MODEL_NAME} \\\n",
        "    --machine-type=\"e2-standard-2\" \\\n",
        "    --min-replica-count='1' \\\n",
        "    --max-replica-count=\"300\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2VtGUv9JOI9"
      },
      "source": [
        "### 5d. Generate embeddings in Google Earth Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "160PNcRRJMzn",
        "outputId": "52266346-f83f-49e9-e487-ba1c96a2dc0b"
      },
      "outputs": [],
      "source": [
        "GEE_SCRIPT_URL = \"https://code.earthengine.google.com/c239905f788f67ecf0cee42753893d1c\"\n",
        "print(f\"Open this script: {GEE_SCRIPT_URL} and use the below string for the ENDPOINT variable\")\n",
        "print(\"Use the below string for the ENDPOINT variable\")\n",
        "print(f\"projects/{PROJECT}/locations/{REGION}/endpoints/{ENDPOINT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PnhA3gfHSrY"
      },
      "source": [
        "### 5e. Undeploy model from endpoint\n",
        "\n",
        "Once predictions are made, you must <strong>undeploy your model</strong> to stop incurring further charges.\n",
        "\n",
        "This can be done using the below code or by using the Google Cloud console directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvOO_sfQDWPt",
        "outputId": "139a2f1a-73af-4b7a-943e-6f05bae8dbfb"
      },
      "outputs": [],
      "source": [
        "def get_deployed_model():\n",
        "    deployed_models = !gcloud ai endpoints describe {ENDPOINT_NAME} --region={REGION} --format 'get(deployedModels)'\n",
        "    if deployed_models[1] == '':\n",
        "        print(\"No models deployed\")\n",
        "    else:\n",
        "        print(deployed_model_id)\n",
        "        return eval(deployed_models[1])['id']\n",
        "\n",
        "deployed_model_id = get_deployed_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vswFTu9kFeHy",
        "outputId": "dcc12b61-5cad-4002-e0d9-33923cd4fbea"
      },
      "outputs": [],
      "source": [
        "!gcloud ai endpoints undeploy-model {ENDPOINT_NAME} --region={REGION} --deployed-model-id={deployed_model_id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9Fq6yspF2ye",
        "outputId": "ac70f2cc-4b0a-4fa7-c595-81ee9a8a276d"
      },
      "outputs": [],
      "source": [
        "get_deployed_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
